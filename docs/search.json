{
  "articles": [
    {
      "path": "index.html",
      "title": "NMFS OpenSci JupyterHub",
      "description": "Testing JupyterHubs for Python and R development environments.\n",
      "author": [],
      "contents": "\n\nContents\nSetting up Git to remember you\nWhat is a JupyterHub\n\nI have set us up a JupyterHub/RStudio cloud-computing hub on Azure. It’s on Kubernetes and will spin up VMs as needed. The VMs are not huge: 2CPU & 8 Gig RAM.\nhttps://jhub.opensci.live/hub/login\nLogin via GitHub. Only members of the JHub GitHub team can log on. This is a testing environment. Contact Eli if you want to test it out.\nInstructions: It should be pretty self-explanatory.\nChose Python or R\nIt is based on the Openscapes docker images and is fairly full-featured but during testing let me know any libraries you need loaded.\nDoes your work persist? Yes. It should be like your computer.\nIs there a limit to storage? Yes. I don’t know what it is. Go ahead an use it so I can get a sense of storage needs.\nHow do I link to my GitHub repositories? Follow these instructions https://snowex-2022.hackweek.io/preliminary/git.html\nPost any comments in the discussion so we all can follow them.\nSetting up Git to remember you\nBasically\nOpen up a terminal\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"yourname@your.com\"\ngit config --global credential.helper store\nMake a PAT on GitHub\nHead back to the JupyterHub and do a push. It’ll ask for your username and password. After that it will remember.\nWhat is a JupyterHub\nRead about why cloud compute environments are great. This one is the same as the SnowEx Hackweek one except that it also has RStudio server: https://snowex-2022.hackweek.io/preliminary/jupyterhub.html\n\n\n\n",
      "last_modified": "2023-09-30T15:02:03+01:00"
    },
    {
      "path": "JHub-User-Guide.html",
      "title": "User Instructions",
      "description": "How to use the hub\n",
      "author": [],
      "contents": "\n\nContents\nLogin\nSpin up your server\nChoose your platform\nLet’s choose RStudio\nClone a repo\nTell Git who you are\n\nLet’s choose JupyterLab\nTell Git who you are\nClone a Git repo\n\nStop your server\n\nLogin\nhttps://jhub.opensci.live/hub/login\nYou will see this. Choose Python for Python only; choose R for Python and R.\n\nSpin up your server\nYou will see this as your server spins up\n\nChoose your platform\nYou can code in RStudio, JupyterLab or Terminal.\n\nLet’s choose RStudio\n\nClone a repo\nChoose ‘new project’ (top right) and Version Control.\n\nTell Git who you are\nTell Git who you are and save your authentication info. You only do this once (or until your PAT expires). Run this code from the R console (not terminal).\nusethis::use_git_config(user.name = \"YourName\", user.email = \"your@email.com\")\nNow create a personal access token for authentication. SAVE the token because you will need it in the next step.\nusethis::create_github_token() \nNow run this and paste in the token.\ngitcreds::gitcreds_set()\nRestart R. You can chose your project from the dropdown on the top right to do this.\nNow commit a change and push.\nNote, you can also run the commands below from a terminal window.\ngit config --global user.name \"YourName\"\ngit config --global user.email \"your@email.com\"\ngit config --global credential.helper store\nLet’s choose JupyterLab\nAny of the browser tabs with the Jupyter icon are JupyterLab.\nBe careful because you can be in RStudio with one GitHub repository and you could open the same repo in JupyterLab and easily create merge conflicts. Just be aware that they are in separate file systems so changes on RStudio will not be reflected in JupyterLab. It is not like you are on one computer.\nTell Git who you are\nBut I just did that with RStudio! I know but the JLab instance is in a different environment and doesn’t know what you did in the RStudio environment.\nOpen a terminal. You do this from the Launcher window. You can always open a new launcher window by clicking the little + tab\n\nNow click Terminal and run this code\ngit config --global user.name \"YourName\"\ngit config --global user.email \"your@email.com\"\ngit config --global credential.helper store\nCreate a PAT or use the one you created for RStudio\nMake a commit and push with the PAT as the password. Now you are set (until your PAT expires).\nClone a Git repo\nClick the Git icon on the left and you can clone a repo.\n\nStop your server\nIt will stop on it’s own after awhile, but if it hangs, you can stop it and restart it. With File > Hub Control Panel\n\n\n\n",
      "last_modified": "2023-09-30T15:02:04+01:00"
    },
    {
      "path": "Set-up-centos.html",
      "title": "Centos Set-up",
      "description": "Setting up a multi-user JupyterHub on Centos 7 VM\n",
      "author": [],
      "contents": "\n\nContents\nCheck Set-up\nCreate the conda environment\nOpen the 8000 port\nSet up a configuration file\nMake a new server service\nCreate the new unit file\nMake sure SELinux doesn’t block our service\n\nStart our new service\nCreate the base user environment\nCheck that the versions of jupyterhub match\nEdit the config file\n\n\nThis is my notes for setting this up on a DreamCloud Centos 7 (Linux distribution) VM.\nAll the commands are run in a shell (bash)\nReferences:\nhttps://jupyterhub.readthedocs.io/en/1.2.0/installation-guide-hard.html This is an old version (1.2.0) while the newest verson of JupyterHub is 4+. However the 4+ docs do not cover a simple bare metal (own server installation).\nhttps://jupyterhub-dockerspawner.readthedocs.io/en/latest/install.html\nCheck Set-up\nLog into your machine (server or VM) as root and get to a terminal (bash) window. How you do this is going to vary. For example, if you have a physical computer with Linux, you will login and open a Terminal. If you are on an network, you would ssh (secure shell) into the machine. On a VM, you would ssh in or often the VM provider will have a widget to get to the shell on the VM.\nYou will need Python 3.6+ installed. Open a terminal window and run\npython3 -version or python -version to see if Python is installed and what the version is.\nCheck your operating system (OS) with\ncat /etc/os-release\nYou will need conda (or miniconda) for these instructions. conda (and miniconda) take care of checking that all our packages will be inter-operable. It is best to install JupyterHub into a clean environment. That way you minimize chances of conflicts and your environment will solve (figure out any conflicts) much much faster.\nCheck for conda with\nconda -V\nInstall miniconda if needed (instructions). Read about miniconda for scientists from Software Carpentries here.\nCreate the conda environment\nCreate the conda environment (which includes a directory with all the files for packages), activate it (enter it), and get the location of the environment (folder).\nAll the commands below are in the terminal window on your machine.\nCreate the environment named jupyterhub with Python 3.9 and jupyterhub (module). Replace 3.9 with the version you have but needs to be 3.6+. After creating, activate (enter) that environment. Then install jupyterlab, notebook and dockerspawner into the environment. Note the jupyterhub after -n is the name of the environment.\nconda create -n jupyterhub python=3.9\nconda activate jupyterhub\nconda install -c conda-forge jupyterhub\nconda install -c conda-forge jupyterlab notebook dockerspawner\nThe environment has a folder with all the packages and binaries that we install. We are going to need to know the location of that folder. Get the location with\nconda env list\nOn the computer I am set up using conda to create the environment, the folder location is\n/opt/jupyterhub/\nYours could be something entirely different. On another server with anaconda (a not-free conda package resolver), the folder was\n/SHARE/anaconda3/envs/jupterhub/\nWe are going to be saving the configuration files for our JupyterHub in this folder. Let’s save the path to a variable so we don’t have to keep entering the whole path.\nJHUB-ENV=/opt/jupyterhub/\nYou should now be able to start the hub, but you will not be able to access it yet. Type\njupyterhub\nand check that it starts. Then use Cntl-C to stop the hub.\nOpen the 8000 port\nFind out the Public IP address for the server you are on. Then open the 8000 port.\nadd port\nreload\nlist ports to check\nBackground\nThe JupyterhHub is running by default on http://localhost:8000. This means that if you start the hub on a machine that you are logged into, you should be able to open a browser on that machine, enter http://localhost:8000 and the hub login page will appear. There are a few reasons that might not work\nYou are ssh-ing into a server and don’t have a browser to open. The browser on the computer that you are ssh-ing from is the “localhost” in this case and you need the “localhost” to be the server.\nYou are logged directly into your server, but it doesn’t have a browser installed.\nHowever http://localhost:8000 is actually not very useful. We are trying to create a hub that others can log into from their browsers.\nSo you need to determine the Public IP address for the server you are on. This is the IP address that you could enter into a browser. If you enter http://XXX.XX.XX.XX (replace with actual IP), then you should see a page of some sort. This indicates that the server is working. If you are on an internal network, then you will only be able to load the address if you are also on that network. But for security reason, ports will not be open by default. You need to open the 8000 port so that http://XXX.XX.XX.XX:8000 will be found.\nSet up a configuration file\nSo far, we have started the hub with the default configuration. We are going to need to customize it. For that we need a configuration file. We will create this in the folder where the environment files are.\nsudo mkdir -p $JHUB-ENV/etc/jupyterhub/\ncd $JHUB-ENV/etc/jupyterhub/\nNext create the default configuration file jupyterhub_config.py.\nsudo $JHUB-PATH/bin/jupyterhub --generate-config\nBecause we cd-d into the $JHUB-PATH/etc/jupyterhub/ directory, the file is created there. This default file is very long and we won’t edit it quite yet.\nMake a new server service\nCreate the new unit file\nAt this point, after opening the port, you should be able to get to your JupyterHub by starting it with jupyterhub --ip XXX.XX.XX.XX --port=8000 and then browsing to http://XXX.XX.XX.XX:8000. But you hub is going to be stopped whenever the server is rebooted. So next we need to set up a service for your service so that our hub starts automatically.\nCreate a new directory for the service unit file,\nsudo mkdir -p $JHUB-PATH/etc/systemd\ncd $JHUB-PATH/etc/systemd\nCreate the file. For example, using nano editor, we do\nnano jupyterhub.service\nAnd into that file we put the following. Replace Environment=\"JHUB-ENV=\\opt\\jupyterhub\\\" with the actual path to the jupyterhub environment folder.\n[Unit]\nDescription=JupyterHub\nAfter=syslog.target network.target\n\n[Service]\nUser=root\nEnvironment=\"JHUB-ENV=/opt/jupyterhub/\"\nEnvironment=\"PATH=/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:$JHUB-ENV/bin\"\nExecStart=$JHUB-ENV/bin/jupyterhub -f $JHUB-ENV/etc/jupyterhub/jupyterhub_config.py\n\n[Install]\nWantedBy=multi-user.target\nNext we make systemd aware of the new service.\nCreate a symlink file in the folder where all the server services are kept. And tell systemd to reload its configuration files\nsudo ln -s $JHUB-ENV/etc/systemd/jupyterhub.service /etc/systemd/system/jupyterhub.service\nsudo systemctl daemon-reload\nMake sure SELinux doesn’t block our service\nSELinux (security for the server) checks that files that are used have the correct label. All our files have generic file labels. If you do,\nls -Z $JHUB-ENV/etc/systemd/\nYou will see that the file label is xyz. We need it to be\nsystemd_unit_file_t\nWe change the file label with\nsudo chron .... $JHUB-PATH/etc/systemd/jupyterhub.service\nSELinux will also object to the file label on all the binaries that we use to start up the JupyterHub (like jupyterhub) so we need to fix those file labels.\nThis will add bin_t label to all the binaries.\n\nStart our new service\nThe service will start on reboot, but we can start it straight away using start:\nsudo systemctl enable jupyterhub.service\nsudo systemctl start jupyterhub.service\nCheck that it is running.\nsudo systemctl status jupyterhub.service\nNow our hub should be available on http:\\\\XXX.XX.XX.XX:8000. At this point, you will need to address security if your hub is open to the web, as opposed to being on an internal network and only accessible to that network. Learn about that here.\nCreate the base user environment\nWhen you log in the jupyter notebooks will be trying to use the Python environment that was created to install JupyterHub, this is not what we want. We will use a docker image to “spawn” the user environment. Read here for other approaches.\nWe are going to use dockerspawner so that we can use a docker image for our user environments. The user will work in these containerized environments and they won’t have access to any other files in the server. In order to share their work with others, the normal workflow would be to work in Git repos and share those repos to a GitHub (or GitLab server). Each user will have a home directory on the server for their files, but they won’t have access to other hub user directories nor will they have access to any other directories on the server.\nCheck that the versions of jupyterhub match\nThe image that we use must have the jupyterhub module installed and the version of jupyterhub on your server must match that in the image.\nCheck the version on your server:\njupyterhub -V\nFor demo purposes, we will use the jupyter/scipy-notebook images on DockerHub. We want to find an image with the same version of jupyterhub as we have on our server.\nEdit the config file\nEdit the jupyterhub_config.py file in $JHUB-ENV/etc/jupyterhub/ to add that we want to use DockerSpawner and specify the image. Add these lines to jupyterhub_config.py.\nc.JupyterHub.spawner_class = 'dockerspawner.DockerSpawner'\nc.DockerSpawner.image = 'jupyter/scipy-notebook:67b8fb91f950'\n\n\n\n",
      "last_modified": "2023-09-30T15:02:04+01:00"
    },
    {
      "path": "Set-up-daskhub.html",
      "title": "DaskHub Set-up",
      "description": "Setting up a multi-user JupyterHub with Dask enabled\n",
      "author": [],
      "contents": "\n\nContents\nCreate your Kubernetes cluster\nInstall DaskHub on your cluster\nConnect to your cluster\nCreate dconfig.yaml\nInstall daskhub via helm chart\nSet-up your external IP address\n\nStep 2 Set up https\nCreate a domain name\nCreate a DNS entry\nTest if the url is working\nSet-up https on your JupyterHub\nUpdate the JupyterHub installation\nTest if https is working\n\nStep 3 Set up GitHub authentication\nCreate a new Oauth Application on GitHub\nCreate a team in your GitHub org\nEdit the dconfig.yaml file\nUpdate the hub\nTest\n\nSet up the container image\nUpdate the hub\n\nChanging the VM size\nCreate a separate disk for user data\nCreate disk\nPVC\nPV\nTell the hub about the disk\n\nTroubleshooting\nRefs I used\nOverall\nStorage\n\nSetting up a shared data disk\nS3 access\n\nThis is my notes for setting this up on Azure. Attempting to replicate the Openscapes 2i2c JupyterHub: https://github.com/2i2c-org/infrastructure/tree/master/config/clusters/openscapes\nThat hub is on AWS and is designed for large workshops (100+) however the NMFS OpenSci JHub is quite similar. Main difference at the moment is that I don’t have a shared drive set-up and the user persistent volume (storage) is on the same VM as the user node for their Jupyter Notebook. This means that I cannot have multiple VM sizes. Need to fix so that user can pick a larger VM for a task if needed.\nCreate your Kubernetes cluster\nLog into https:\\\\portal.azure.com\nGet to the dashboard that looks similar to this.\n\nClick on the Kubernetes Services button and you should see something like this\n\nClick Create Kubernetes Cluster\nAt this point, you will get to the set-up with lots of tabs.\nYou need to select the resource group if you are in a subscription for an organization. Don’t know what resource group to use, ask the admins.\nYou need to give your Kubernetes cluster a name. For example, jhub or daskhub or whatever.\nYou need to chose the AWS region. If you are using AWS S3 file access (big data in the cloud), then you need to be on the same region as the files you are accessing. Do you have no idea? Then you are probably not using AWS S3 file access. In that case, just go with the default or something close to you.\nNext you chose the “Node size”. This is the size of the base virtural machine (VM). It is going to spin up as many as it needs. The default is Standard DS2 v2 which as 2 CPU, 7 Gig RAM and 1T memory. This is fine for set-up. You can add more (bigger VMs later). Accept autoscaling since this is a multi-user hub.\nThe first tab is all you need for now. Later you may want to allow the user, to choose a different base VM. You can do that by adding node-pools. That’ll be covered after the initial set-up. For now, just get your basic hub working. You can add more VM sizes later.\nClick “Review and Create”\nWait for validation tests to pass.\nClick “Create”.\nOnce it is done deploying, you will see this.\n\nInstall DaskHub on your cluster\nThese next steps are done in the shell after connecting to your cluster. First you need to get to the shell.\nConnect to your cluster\nOnce you have created your Kubernetes cluster, you want to go to its dashboard (by clicking on the name you gave it). You’ll see something like this (I named mine daskhub).\n\nClick on the Connect icon in the nav bar at top.\nYou then see this\n\nClick on the link that says “Open Cloud Shell”.\n\nYou will get to a terminal. Paste in the two commands in the previous image (the commands that show up for you that is).\nCreate dconfig.yaml\nThis will be the configuration file for your Dask-enabled JupyterHub. For now, it can be just comments. Note the name is unimportant but should end in .yaml. I am using dconfig.yaml instead of config.yaml since I already have a config.yaml file for something else–and I have not figured out how to install different hubs in different directories or even different clusters in different directories (I have much to learn…).\nnano dconfig.yaml\nThis will open the nano editor. Edit your file. You can do # just blank for now. Then Cntl-O to save and Cntl-X to exit.\nInstall daskhub via helm chart\nInstructions: https://artifacthub.io/packages/helm/dask/daskhub .\nCheck that helm is installed\nhelm version\nTell helm about the dask helm repository\nhelm repo add dask https://helm.dask.org\nhelm repo update\nNow install\nhelm upgrade --wait --install --render-subchart-notes \\\n    dhub dask/daskhub \\\n    --namespace=dhub --create-namespace \\\n    --values=dconfig.yaml\nYou will see this on successful installation (it’s long. much has been cut). \nSet-up your external IP address\nkubectl config set-context $(kubectl config current-context) --namespace dhub\nkubectl --namespace=dhub get service proxy-public\nThese commands will show the the IP address. Save the public IP address. You will need it in step 2. Look for the IP address under EXTERNAL-IP.\nStep 2 Set up https\nYou can log out of your cluster. The next steps are done elsewhere.\nCreate a domain name\nYou will need a domain name for https which you want for security (and JHub won’t stop complaining if you don’t). Find a domain name provider and set one up. It is not expensive. I used GoDaddy.\nCreate a DNS entry\nLet’s pretend you set up bluemountain123.live as the domain. Go to the DNS settings for your domain. Add a type A record. This will do 2 things. First this will create the subdomain that you will use to access your JupyterHub. So let’s say you create, dhub as the type A DNS entry. Then dhub.bluemountain123.live will be the url. You can have as many subdomains as you need.\n\nTest if the url is working\nhttp:\\\\dhub.bluemountain123.live would be the url using the example domain above. Test that it is working (shows a JupyterHub login) before moving on. This is what you should see:\n\nSet-up https on your JupyterHub\nLog back into your Kubernetes cluster: go to portal.azure.com, click on your Kubernetes cluster name, and then click on “Connect”. Then click on “Open Cloud Shell”. Read documentation about https\nOnce you are on the shell, type\nnano dconfig.yaml\nto edit the config file. Paste this in and save. Note the additional jupyterhub: in the yaml file. This is not in a plain JupyterHub with Kubernetes config file (i.e. in a non-daskhub, the jupyterhub: bit is not there and everything is moved to left by 2 spaces).\njupyterhub:\n  proxy:\n    https:\n      enabled: true\n      hosts:\n        - dhub.bluemountain123.live\n      letsencrypt:\n        contactEmail: your@email.com\nUpdate the JupyterHub installation\nAnytime you change dconfig.yaml you need to run this code.\nhelm upgrade --cleanup-on-fail --render-subchart-notes dhub dask/daskhub --namespace dhub --version=2023.1.0 --values dconfig.yaml\nTest if https is working\nTry https:\\\\dhub.bluemountain123.live and you should see the JupyterHub login without that http warning.\nStep 3 Set up GitHub authentication\nOptional, if you want to manage who can login via GitHub Team. I am going to show an example where I use a team on a GitHub organization to manage authentication. There are many other ways to manage users. Google to find that.\nCreate a new Oauth Application on GitHub\nThis is going to be associated with your (personal) GitHub account, but you can use a team on a GitHub org that you are owner of.\nLog into GitHub and go to GitHub > Settings > Developer Settings > New Oauth Application\nLook carefully at how I filled in the boxes.\n\nNext you will see something like this\n\nYou need to copy the ID and then click the create secrets button and save the secret. Save those for later.\nCreate a team in your GitHub org\nYou will be added by default and add anyone else who needs access to the hub. Let’s say your org is MyOrg and the team is called DaskHub. So then the allowed organization is MyOrg:DaskHub. You can leave off :DaskHub if you want to allow all members of the organization to log in.\nEdit the dconfig.yaml file\nnano dconfig.yaml\nAdd to your config file so it is now this. Replace the id, secret and url with your values. We need to set the KubeSpawner working directory because the Openscapes Docker image sets it to home/jovyan/.kernels–which is fine but annoying since .kernels is hidden and not $HOME.\njupyterhub:\n  hub:\n    config:\n      GitHubOAuthenticator:\n        client_id: <replace with your OAuth id>\n        client_secret: <replace with your OAuth app secret>\n        oauth_callback_url: https://dhub.bluemountain123.live/hub/oauth_callback\n        allowed_organizations:\n          - MyOrg:DaskHub\n        scope:\n          - read:org\n      JupyterHub:\n        authenticator_class: github\n      KubeSpawner:\n        working_dir: /home/jovyan\n  proxy:\n    https:\n      enabled: true\n      hosts:\n        - dhub.bluemountain123.live\n      letsencrypt:\n        contactEmail: your@email.com        \nUpdate the hub\nhelm upgrade --cleanup-on-fail --render-subchart-notes dhub dask/daskhub --namespace dhub --version=2023.1.0 --values dconfig.yaml\nTest\nYou should now see this and can authenticate with GitHub.\n\nSet up the container image\nNow you need to specify the Docker image that will be used. We will use 2 different profiles: Python and R (RStudio).\nEdit the dconfig.yaml file and add the user image info. Note the spacing matters (a lot). I also added some Dask gateway config.\njupyterhub:\n  hub:\n    config:\n      GitHubOAuthenticator:\n        client_id: <replace with your OAuth id>\n        client_secret: <replace with your OAuth app secret>\n        oauth_callback_url: https://dhub.bluemountain123.live/hub/oauth_callback\n        allowed_organizations:\n          - MyOrg:DaskHub\n        scope:\n          - read:org\n      JupyterHub:\n        authenticator_class: github\n  proxy:\n    https:\n      enabled: true\n      hosts:\n        - dhub.bluemountain123.live\n      letsencrypt:\n        contactEmail: your@email.com        \n  singleuser:\n    image:\n      name: openscapes/python\n      tag: f577786\n    cmd: null\n  singleuser:\n    # Defines the default image\n    image:\n      name: openscapes/python\n      tag: f577786\n    profileList:\n      - display_name: \"Python3\"\n        description: \"NASA Openscapes Python image\"\n        default: true\n      - display_name: \"R\"\n        description: \"NASA Openscapes RStudio image\"\n        kubespawner_override:\n          image: openscapes/rocker:a7596b5        \ndask-gateway:\n  gateway:\n    extraConfig:\n      idle: |-\n        # timeout after 30 minutes of inactivity\n        c.KubeClusterConfig.idle_timeout = 1800        \nUpdate the hub\nhelm upgrade --cleanup-on-fail --render-subchart-notes dhub dask/daskhub --namespace dhub --version=2023.1.0 --values dconfig.yaml\nChanging the VM size\nNOT WORKING YET I am stuck on creating the persistent volumes. Needed because you need the user storage somewhere if you have multiple node pools.\n\nkubectl get nodes --show-labels | grep instance-type\nbeta.kubernetes.io/instance-type=Standard_D8s_v3\nCreate a separate disk for user data\nI want the user data to be in a drive different from the VM being spun up for their notebook. Sounds easy here https://z2jh.jupyter.org/en/latest/jupyterhub/customizing/user-storage.html\nbut I cannot string the steps together.\nSteps, I think?\nCreate disk\nSomething like this?\nhttps://bluexp.netapp.com/blog/azure-cvo-blg-azure-kubernetes-service-configuring-persistent-volumes-in-aks\nBut I can’t figure out the steps.\nPVC\nNOT WORKING YET\nIs this pvc.yaml right?\nHow would I point this to the disk that I mount in the step above??\nThis command might have useful info\nKUBE_EDITOR=\"nano\" kubectl edit pvc --namespace=dhub claim-eeholmes\nnana pvc.yaml\nkind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: hub-db-dir\n  labels:\n    component: jupyter\nspec:\n  storageClassName: \"standard\" # name of storage class, it will be default storage class if unspecified.\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: \"40Gi\"\nkubectl create -f pvc.yaml\nTo delete, you need to first edit the pvc yaml file and get rid of pvc protection. It is 2 lines.\nkubectl --namespace=dhub get pvc\nKUBE_EDITOR=\"nano\" kubectl edit pvc --namespace=dhub claim-eeholmes\nThen you can delete\nkubectl --namespace=dhub delete pvc claim-eeholmes\nCheck that it is gone\nkubectl --namespace=dhub get pvc\nif not try\nkubectl --namespace=dhub delete pvc claim-eeholmes --grace-period=0 --force\nPV\nNeed a persistent volume claim too….\nTell the hub about the disk\nhttps://z2jh.jupyter.org/en/latest/jupyterhub/customizing/user-storage.html\nBut see how this is done on the Openscapes 2i2c hub https://github.com/2i2c-org/infrastructure/blob/master/config/clusters/openscapes/common.values.yaml\nI know their set-up is a little different: basehub -> jupyterhub in the helm chart, but I don’t see how the singleuser bit in the yaml file is referencing the nfs in the top of that yaml.\nTroubleshooting\nI cannot clone repos in the JupyterHub. Restart the server. In Jupyter, File > Hub Control Panel > Stop My Server.\nRefs I used\nOverall\nhttps://2i2c.org/service/#getahub\nOpenscapes common.values.yaml https://github.com/2i2c-org/infrastructure/blob/master/config/clusters/openscapes/common.values.yaml\nhttps://artifacthub.io/packages/helm/dask/daskhub\nhttps://github.com/zonca/jupyterhub-deploy-kubernetes-jetstream/blob/master/dask_gateway/dask-hub/config_daskhub.yaml\nhttps://saturncloud.io/blog/how-to-setup-jupyterhub-on-azure/\nhttps://saturncloud.io/blog/jupyterhub-and-azure-ad/\nStorage\nhttps://www.youtube.com/watch?v=Da1qn7-RHvY\nDynamic NFS provisioning 2 https://www.youtube.com/watch?v=DF3v2P8ENEg&t=0s\nDynamic NFS provisioning 1 https://www.youtube.com/watch?v=AavnQzWDTEk&t=0s\nhttps://alan-turing-institute.github.io/hub23-deploy/\nhttps://z2jh.jupyter.org/en/latest/jupyterhub/customizing/user-storage.html\nhttps://learn.microsoft.com/en-us/azure/aks/azure-nfs-volume\nhttps://learn.microsoft.com/en-us/azure/storage/files/storage-files-quick-create-use-linux\nhttps://bluexp.netapp.com/blog/azure-cvo-blg-azure-kubernetes-service-configuring-persistent-volumes-in-aks\nSetting up a shared data disk\nhttps://www.mathworks.com/help/matlab/import_export/work-with-remote-data.html\nhttps://realpython.com/storing-images-in-python/\nS3 access\nhttps://s3fs.readthedocs.io/en/latest/\nhttps://stackoverflow.com/questions/67259323/jupyterhub-access-aws-s3\nhttps://data.lpdaac.earthdatacloud.nasa.gov/s3credentialsREADME\n\n\n\n",
      "last_modified": "2023-09-30T15:02:05+01:00"
    },
    {
      "path": "Setup-Notes.html",
      "author": [],
      "contents": "\n\n\n\n\n\n\n\n\n  \n    \n      \n        \n        \n        \n      \n      \n    \n    \n      \n  User Guide\n\n\n  Hub Set-up\n\n\n  Centos Set-up\n\n      \n  \n\n\n\n\n\n\n\n\n\n\n\nInstructions for editing config\nLog into https://portal.azure.com/ and once successful, you will\nsee this\n\nClick the JupyterHub icon and you will see this\n\nClick the Connect icon and you will see this. Ignore everything else\nthat you see. I don’t think you need to run the\nkubectl get deployments --all-namespaces=true unless you\nneed to check Kubernetes set up.\n\nType nano config.yaml to get the the JupyterHub config.\nThis is the only file you need to change. cntl-O to write. cntl-X to\nexit.\nAfter you update the config.yaml, you need to tell the\nJupyterHub about the change\nhelm upgrade --cleanup-on-fail jhub jupyterhub/jupyterhub --namespace jhub  --version=2.0.0 --values config.yaml\nIf upgrade was successful, you will see this (plus a bunch of text\nbelow that you can ignore).\n\nWhat a few minutes for your changes to take effect.\n\n\n\n\n\n\n\n",
      "last_modified": "2023-09-30T15:02:05+01:00"
    }
  ],
  "collections": []
}
