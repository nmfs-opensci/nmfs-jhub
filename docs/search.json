[
  {
    "objectID": "Set-up-daskhub.html",
    "href": "Set-up-daskhub.html",
    "title": "DaskHub Set-up",
    "section": "",
    "text": "This is my notes for setting this up on Azure. Attempting to replicate the Openscapes 2i2c JupyterHub: https://github.com/2i2c-org/infrastructure/tree/master/config/clusters/openscapes\nThat hub is on AWS and is designed for large workshops (100+) however the NMFS OpenSci JHub is quite similar. Main difference at the moment is that I don’t have a shared drive set-up and the user persistent volume (storage) is on the same VM as the user node for their Jupyter Notebook. This means that I cannot have multiple VM sizes. Need to fix so that user can pick a larger VM for a task if needed."
  },
  {
    "objectID": "Set-up-daskhub.html#create-your-kubernetes-cluster",
    "href": "Set-up-daskhub.html#create-your-kubernetes-cluster",
    "title": "DaskHub Set-up",
    "section": "Create your Kubernetes cluster",
    "text": "Create your Kubernetes cluster\nLog into https:\\\\portal.azure.com\n\nGet to the dashboard that looks similar to this.\n\n\n\nClick on the Kubernetes Services button and you should see something like this\n\n\n\nClick Create Kubernetes Cluster\n\nAt this point, you will get to the set-up with lots of tabs.\n\nYou need to select the resource group if you are in a subscription for an organization. Don’t know what resource group to use, ask the admins.\nYou need to give your Kubernetes cluster a name. For example, jhub or daskhub or whatever.\nYou need to chose the AWS region. If you are using AWS S3 file access (big data in the cloud), then you need to be on the same region as the files you are accessing. Do you have no idea? Then you are probably not using AWS S3 file access. In that case, just go with the default or something close to you.\nNext you chose the “Node size”. This is the size of the base virtural machine (VM). It is going to spin up as many as it needs. The default is Standard DS2 v2 which as 2 CPU, 7 Gig RAM and 1T memory. This is fine for set-up. You can add more (bigger VMs later). Accept autoscaling since this is a multi-user hub.\n\nThe first tab is all you need for now. Later you may want to allow the user, to choose a different base VM. You can do that by adding node-pools. That’ll be covered after the initial set-up. For now, just get your basic hub working. You can add more VM sizes later.\n\nClick “Review and Create”\n\nWait for validation tests to pass.\n\nClick “Create”.\n\nOnce it is done deploying, you will see this."
  },
  {
    "objectID": "Set-up-daskhub.html#install-daskhub-on-your-cluster",
    "href": "Set-up-daskhub.html#install-daskhub-on-your-cluster",
    "title": "DaskHub Set-up",
    "section": "Install DaskHub on your cluster",
    "text": "Install DaskHub on your cluster\nThese next steps are done in the shell after connecting to your cluster. First you need to get to the shell.\n\nConnect to your cluster\nOnce you have created your Kubernetes cluster, you want to go to its dashboard (by clicking on the name you gave it). You’ll see something like this (I named mine daskhub).\n\nClick on the Connect icon in the nav bar at top.\nYou then see this\n\nClick on the link that says “Open Cloud Shell”.\n\nYou will get to a terminal. Paste in the two commands in the previous image (the commands that show up for you that is).\n\n\nCreate dconfig.yaml\nThis will be the configuration file for your Dask-enabled JupyterHub. For now, it can be just comments. Note the name is unimportant but should end in .yaml. I am using dconfig.yaml instead of config.yaml since I already have a config.yaml file for something else–and I have not figured out how to install different hubs in different directories or even different clusters in different directories (I have much to learn…).\nnano dconfig.yaml\nThis will open the nano editor. Edit your file. You can do # just blank for now. Then Cntl-O to save and Cntl-X to exit.\n\n\nInstall daskhub via helm chart\nInstructions: https://artifacthub.io/packages/helm/dask/daskhub .\nCheck that helm is installed\nhelm version\nTell helm about the dask helm repository\nhelm repo add dask https://helm.dask.org\nhelm repo update\nNow install\nhelm upgrade --wait --install --render-subchart-notes \\\n    dhub dask/daskhub \\\n    --namespace=dhub --create-namespace \\\n    --values=dconfig.yaml\nYou will see this on successful installation (it’s long. much has been cut). \n\n\nSet-up your external IP address\nkubectl config set-context $(kubectl config current-context) --namespace dhub\nkubectl --namespace=dhub get service proxy-public\nThese commands will show the the IP address. Save the public IP address. You will need it in step 2. Look for the IP address under EXTERNAL-IP."
  },
  {
    "objectID": "Set-up-daskhub.html#step-2-set-up-https",
    "href": "Set-up-daskhub.html#step-2-set-up-https",
    "title": "DaskHub Set-up",
    "section": "Step 2 Set up https",
    "text": "Step 2 Set up https\nYou can log out of your cluster. The next steps are done elsewhere.\n\nCreate a domain name\nYou will need a domain name for https which you want for security (and JHub won’t stop complaining if you don’t). Find a domain name provider and set one up. It is not expensive. I used GoDaddy.\n\n\nCreate a DNS entry\nLet’s pretend you set up bluemountain123.live as the domain. Go to the DNS settings for your domain. Add a type A record. This will do 2 things. First this will create the subdomain that you will use to access your JupyterHub. So let’s say you create, dhub as the type A DNS entry. Then dhub.bluemountain123.live will be the url. You can have as many subdomains as you need.\n\n\n\nTest if the url is working\nhttp:\\\\dhub.bluemountain123.live would be the url using the example domain above. Test that it is working (shows a JupyterHub login) before moving on. This is what you should see:\n\n\n\nSet-up https on your JupyterHub\nLog back into your Kubernetes cluster: go to portal.azure.com, click on your Kubernetes cluster name, and then click on “Connect”. Then click on “Open Cloud Shell”. Read documentation about https\nOnce you are on the shell, type\nnano dconfig.yaml\nto edit the config file. Paste this in and save. Note the additional jupyterhub: in the yaml file. This is not in a plain JupyterHub with Kubernetes config file (i.e. in a non-daskhub, the jupyterhub: bit is not there and everything is moved to left by 2 spaces).\njupyterhub:\n  proxy:\n    https:\n      enabled: true\n      hosts:\n        - dhub.bluemountain123.live\n      letsencrypt:\n        contactEmail: your@email.com\n\n\nUpdate the JupyterHub installation\nAnytime you change dconfig.yaml you need to run this code.\nhelm upgrade --cleanup-on-fail --render-subchart-notes dhub dask/daskhub --namespace dhub --version=2023.1.0 --values dconfig.yaml\n\n\nTest if https is working\nTry https:\\\\dhub.bluemountain123.live and you should see the JupyterHub login without that http warning."
  },
  {
    "objectID": "Set-up-daskhub.html#step-3-set-up-github-authentication",
    "href": "Set-up-daskhub.html#step-3-set-up-github-authentication",
    "title": "DaskHub Set-up",
    "section": "Step 3 Set up GitHub authentication",
    "text": "Step 3 Set up GitHub authentication\nOptional, if you want to manage who can login via GitHub Team. I am going to show an example where I use a team on a GitHub organization to manage authentication. There are many other ways to manage users. Google to find that.\n\nCreate a new Oauth Application on GitHub\nThis is going to be associated with your (personal) GitHub account, but you can use a team on a GitHub org that you are owner of.\nLog into GitHub and go to GitHub &gt; Settings &gt; Developer Settings &gt; New Oauth Application\nLook carefully at how I filled in the boxes.\n\nNext you will see something like this\n\nYou need to copy the ID and then click the create secrets button and save the secret. Save those for later.\n\n\nCreate a team in your GitHub org\nYou will be added by default and add anyone else who needs access to the hub. Let’s say your org is MyOrg and the team is called DaskHub. So then the allowed organization is MyOrg:DaskHub. You can leave off :DaskHub if you want to allow all members of the organization to log in.\n\n\nEdit the dconfig.yaml file\nnano dconfig.yaml\nAdd to your config file so it is now this. Replace the id, secret and url with your values. We need to set the KubeSpawner working directory because the Openscapes Docker image sets it to home/jovyan/.kernels–which is fine but annoying since .kernels is hidden and not $HOME.\njupyterhub:\n  hub:\n    config:\n      GitHubOAuthenticator:\n        client_id: &lt;replace with your OAuth id&gt;\n        client_secret: &lt;replace with your OAuth app secret&gt;\n        oauth_callback_url: https://dhub.bluemountain123.live/hub/oauth_callback\n        allowed_organizations:\n          - MyOrg:DaskHub\n        scope:\n          - read:org\n      JupyterHub:\n        authenticator_class: github\n      KubeSpawner:\n        working_dir: /home/jovyan\n  proxy:\n    https:\n      enabled: true\n      hosts:\n        - dhub.bluemountain123.live\n      letsencrypt:\n        contactEmail: your@email.com        \n\n\nUpdate the hub\nhelm upgrade --cleanup-on-fail --render-subchart-notes dhub dask/daskhub --namespace dhub --version=2023.1.0 --values dconfig.yaml\n\n\nTest\nYou should now see this and can authenticate with GitHub."
  },
  {
    "objectID": "Set-up-daskhub.html#set-up-the-container-image",
    "href": "Set-up-daskhub.html#set-up-the-container-image",
    "title": "DaskHub Set-up",
    "section": "Set up the container image",
    "text": "Set up the container image\nNow you need to specify the Docker image that will be used. We will use 2 different profiles: Python and R (RStudio).\nEdit the dconfig.yaml file and add the user image info. Note the spacing matters (a lot). I also added some Dask gateway config.\njupyterhub:\n  hub:\n    config:\n      GitHubOAuthenticator:\n        client_id: &lt;replace with your OAuth id&gt;\n        client_secret: &lt;replace with your OAuth app secret&gt;\n        oauth_callback_url: https://dhub.bluemountain123.live/hub/oauth_callback\n        allowed_organizations:\n          - MyOrg:DaskHub\n        scope:\n          - read:org\n      JupyterHub:\n        authenticator_class: github\n  proxy:\n    https:\n      enabled: true\n      hosts:\n        - dhub.bluemountain123.live\n      letsencrypt:\n        contactEmail: your@email.com        \n  singleuser:\n    image:\n      name: openscapes/python\n      tag: f577786\n    cmd: null\n  singleuser:\n    # Defines the default image\n    image:\n      name: openscapes/python\n      tag: f577786\n    profileList:\n      - display_name: \"Python3\"\n        description: \"NASA Openscapes Python image\"\n        default: true\n      - display_name: \"R\"\n        description: \"NASA Openscapes RStudio image\"\n        kubespawner_override:\n          image: openscapes/rocker:a7596b5        \ndask-gateway:\n  gateway:\n    extraConfig:\n      idle: |-\n        # timeout after 30 minutes of inactivity\n        c.KubeClusterConfig.idle_timeout = 1800        \n\nUpdate the hub\nhelm upgrade --cleanup-on-fail --render-subchart-notes dhub dask/daskhub --namespace dhub --version=2023.1.0 --values dconfig.yaml"
  },
  {
    "objectID": "Set-up-daskhub.html#changing-the-vm-size",
    "href": "Set-up-daskhub.html#changing-the-vm-size",
    "title": "DaskHub Set-up",
    "section": "Changing the VM size",
    "text": "Changing the VM size\nNOT WORKING YET I am stuck on creating the persistent volumes. Needed because you need the user storage somewhere if you have multiple node pools.\n\nkubectl get nodes --show-labels | grep instance-type\nbeta.kubernetes.io/instance-type=Standard_D8s_v3"
  },
  {
    "objectID": "Set-up-daskhub.html#create-a-separate-disk-for-user-data",
    "href": "Set-up-daskhub.html#create-a-separate-disk-for-user-data",
    "title": "DaskHub Set-up",
    "section": "Create a separate disk for user data",
    "text": "Create a separate disk for user data\nI want the user data to be in a drive different from the VM being spun up for their notebook. Sounds easy here https://z2jh.jupyter.org/en/latest/jupyterhub/customizing/user-storage.html but I cannot string the steps together.\nSteps, I think?\n\nCreate disk\nSomething like this?\nhttps://bluexp.netapp.com/blog/azure-cvo-blg-azure-kubernetes-service-configuring-persistent-volumes-in-aks\nBut I can’t figure out the steps.\n\n\nPVC\nNOT WORKING YET\n\nIs this pvc.yaml right?\nHow would I point this to the disk that I mount in the step above??\n\nThis command might have useful info\nKUBE_EDITOR=\"nano\" kubectl edit pvc --namespace=dhub claim-eeholmes\nnana pvc.yaml\nkind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: hub-db-dir\n  labels:\n    component: jupyter\nspec:\n  storageClassName: \"standard\" # name of storage class, it will be default storage class if unspecified.\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: \"40Gi\"\nkubectl create -f pvc.yaml\nTo delete, you need to first edit the pvc yaml file and get rid of pvc protection. It is 2 lines.\nkubectl --namespace=dhub get pvc\nKUBE_EDITOR=\"nano\" kubectl edit pvc --namespace=dhub claim-eeholmes\nThen you can delete\nkubectl --namespace=dhub delete pvc claim-eeholmes\nCheck that it is gone\nkubectl --namespace=dhub get pvc\nif not try\nkubectl --namespace=dhub delete pvc claim-eeholmes --grace-period=0 --force\n\n\nPV\nNeed a persistent volume claim too….\n\n\nTell the hub about the disk\nhttps://z2jh.jupyter.org/en/latest/jupyterhub/customizing/user-storage.html\nBut see how this is done on the Openscapes 2i2c hub https://github.com/2i2c-org/infrastructure/blob/master/config/clusters/openscapes/common.values.yaml\nI know their set-up is a little different: basehub -&gt; jupyterhub in the helm chart, but I don’t see how the singleuser bit in the yaml file is referencing the nfs in the top of that yaml."
  },
  {
    "objectID": "Set-up-daskhub.html#troubleshooting",
    "href": "Set-up-daskhub.html#troubleshooting",
    "title": "DaskHub Set-up",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nI cannot clone repos in the JupyterHub. Restart the server. In Jupyter, File &gt; Hub Control Panel &gt; Stop My Server."
  },
  {
    "objectID": "Set-up-daskhub.html#refs-i-used",
    "href": "Set-up-daskhub.html#refs-i-used",
    "title": "DaskHub Set-up",
    "section": "Refs I used",
    "text": "Refs I used\n\nOverall\n\nhttps://2i2c.org/service/#getahub\nOpenscapes common.values.yaml https://github.com/2i2c-org/infrastructure/blob/master/config/clusters/openscapes/common.values.yaml\nhttps://artifacthub.io/packages/helm/dask/daskhub\nhttps://github.com/zonca/jupyterhub-deploy-kubernetes-jetstream/blob/master/dask_gateway/dask-hub/config_daskhub.yaml\nhttps://saturncloud.io/blog/how-to-setup-jupyterhub-on-azure/\nhttps://saturncloud.io/blog/jupyterhub-and-azure-ad/\n\n\n\nStorage\n\nhttps://www.youtube.com/watch?v=Da1qn7-RHvY\nDynamic NFS provisioning 2 https://www.youtube.com/watch?v=DF3v2P8ENEg&t=0s\nDynamic NFS provisioning 1 https://www.youtube.com/watch?v=AavnQzWDTEk&t=0s\nhttps://alan-turing-institute.github.io/hub23-deploy/\nhttps://z2jh.jupyter.org/en/latest/jupyterhub/customizing/user-storage.html\nhttps://learn.microsoft.com/en-us/azure/aks/azure-nfs-volume\nhttps://learn.microsoft.com/en-us/azure/storage/files/storage-files-quick-create-use-linux\nhttps://bluexp.netapp.com/blog/azure-cvo-blg-azure-kubernetes-service-configuring-persistent-volumes-in-aks"
  },
  {
    "objectID": "Set-up-daskhub.html#setting-up-a-shared-data-disk",
    "href": "Set-up-daskhub.html#setting-up-a-shared-data-disk",
    "title": "DaskHub Set-up",
    "section": "Setting up a shared data disk",
    "text": "Setting up a shared data disk\n\nhttps://www.mathworks.com/help/matlab/import_export/work-with-remote-data.html\nhttps://realpython.com/storing-images-in-python/"
  },
  {
    "objectID": "Set-up-daskhub.html#s3-access",
    "href": "Set-up-daskhub.html#s3-access",
    "title": "DaskHub Set-up",
    "section": "S3 access",
    "text": "S3 access\n\nhttps://s3fs.readthedocs.io/en/latest/\nhttps://stackoverflow.com/questions/67259323/jupyterhub-access-aws-s3\nhttps://data.lpdaac.earthdatacloud.nasa.gov/s3credentialsREADME"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NMFS OpenSci JupyterHub Notes",
    "section": "",
    "text": "This page shows my notes for setting up JupyterHubs on various platforms. What is a JupyterHub? Read about why cloud computing environments are great: SnowEx 2022"
  },
  {
    "objectID": "index.html#test-jhub",
    "href": "index.html#test-jhub",
    "title": "NMFS OpenSci JupyterHub Notes",
    "section": "Test JHub",
    "text": "Test JHub\nI have set us up a JHub with RStudio on Azure. It’s on Kubernetes and will spin up VMs as needed. The VMs are not huge: 2CPU & 8 Gig RAM.\nhttps://jhub.opensci.live/hub/login\nContact Eli if you want to test it out. See instructions under JHub User Guide"
  },
  {
    "objectID": "index.html#installation-instructions",
    "href": "index.html#installation-instructions",
    "title": "NMFS OpenSci JupyterHub Notes",
    "section": "Installation instructions",
    "text": "Installation instructions\nThe other tabs to the left are my JHub installation notes for various platforms."
  },
  {
    "objectID": "ci/iorocker/instructions.html",
    "href": "ci/iorocker/instructions.html",
    "title": "Indian Ocean Summer Docker Images",
    "section": "",
    "text": "https://hub.docker.com/repository/docker/eeholmes/iopython/general\nThe one to use is the dated one. The main tag doesn’t seem to always be recognized as a new tag when it changes."
  },
  {
    "objectID": "ci/iorocker/instructions.html#to-set-docker-tag-to-latest-commit",
    "href": "ci/iorocker/instructions.html#to-set-docker-tag-to-latest-commit",
    "title": "Indian Ocean Summer Docker Images",
    "section": "To set docker tag to latest commit",
    "text": "To set docker tag to latest commit\nSHA7=\"$(git rev-parse --short HEAD)\"\nDOCKER_TAG=$SHA7\nI am not doing that since this repo has lots of commits unrelated to the docker image."
  },
  {
    "objectID": "ci/iorocker/instructions.html#to-set-up-your-own-docker-repo",
    "href": "ci/iorocker/instructions.html#to-set-up-your-own-docker-repo",
    "title": "Indian Ocean Summer Docker Images",
    "section": "To set up your own Docker repo",
    "text": "To set up your own Docker repo\n\nMake an account on DockerHub. Free is fine.\nCreate a repo and give it a name. For example, for this project, my account is eeholmes and my repo is iopython (Indian Ocean Python) as it is specific a particular project I am working on.\n\nDockerHub will want to you to buy the premium account but you only need that if you are doing continuous integration, like using a GitHub Action to autobuild your image. If you are manually building, you don’t need this."
  },
  {
    "objectID": "ci/iorocker/instructions.html#why-is---platform-needed-in-the-build-command",
    "href": "ci/iorocker/instructions.html#why-is---platform-needed-in-the-build-command",
    "title": "Indian Ocean Summer Docker Images",
    "section": "Why is --platform needed in the build command",
    "text": "Why is --platform needed in the build command\nYou won’t see this on docker build tutorials. But if you are on a Mac with Apple chip, then you’ll build arm64 images and that’s not going to work on Ubuntu VMs. The vanilla images you see are amd64 so we want to make sure we are building for that platform. This only matters if you are on a Mac with Apple chip, but it won’t break things for unix and PC so I added to make the instructions more robust."
  },
  {
    "objectID": "ci/iorocker/instructions.html#if-an-specific-image-tag-is-in-config",
    "href": "ci/iorocker/instructions.html#if-an-specific-image-tag-is-in-config",
    "title": "Indian Ocean Summer Docker Images",
    "section": "If an specific image tag is in config",
    "text": "If an specific image tag is in config\nThe JupyterHub has a config file that specifies what images are being used. If the image is say eeholmes/iopython:hublatest, then whenever the a image with tag hublatest is pushed, the hub will use that. If on the otherhand, you config file has a specific, an unique tag that you don’t overwrite, then you’ll have to update the file in the config file on the cluster (log into Azure, go to cluster, connect to cloud shell, nano dconfig2.yaml) and upgrade the installation of the JupyterHub.\nWhy not eeholmes/iopython:latest? There is nothing special about latest. It is the default tag used if you don’t specify -t and : in your build call. So it is a bit too easy to accidentally update “latest” and thus update the image for you hub when you didn’t intend to do that. You just forgot to specify a tag.\nTo update if you are using a specific tag, like 20230615 rather than one you keep updating like hublatest or latest:\n\nStep 1\nEdit the config file. Mine is called dconfig2.yaml. Yours is probably config.yaml. Name is unimportant.\nnano dconfig2.yaml\nInside dconfig2.yaml is this info. This shows a fixed tag. So if I update, I need to change the 20230615 part.\n  singleuser:\n    image:\n      name: eeholmes/iopython\n      tag: 20230615\nSave the changes. In nano, it cmd-O, return, cmd-X.\n\n\nrun helm upgrade\nhelm upgrade --cleanup-on-fail --render-subchart-notes dhub dask/daskhub --namespace dhub --version=2023.1.0 --values dconfig2.yaml\n\n\nThe helm upgrade command\nhelm upgrade --cleanup-on-fail --render-subchart-notes dhub dask/daskhub --namespace dhub --version=2023.1.0 --values dconfig2.yaml\nA helm is what runs the commands to upgrade (and install in the beginning) our JupyterHub. dask/daskhub is point to the repo with the “helm chart” (the instructions). --value dconfig2.yaml is telling it where the config file is.\n\nupgrade upgrade an existing installation with the values in dconfig2.yaml\n--render-subchart-notes the dask/daskhub helm chart has subcharts (jupyterhub) and you need to render these too. Not all helm charts have this.\ndask/daskhub the name of the repo that has the helm chart. The first time you reference this, you need to tell help about the repo by giving it the url. Read how here\n--version=2023.1.0 version of the helm chart. Update when the helm chart (instructions for installing the jupyterhub) changes."
  },
  {
    "objectID": "ci/iorocker/instructions.html#adding-packages-with-newpackages.yml",
    "href": "ci/iorocker/instructions.html#adding-packages-with-newpackages.yml",
    "title": "Indian Ocean Summer Docker Images",
    "section": "Adding packages with newpackages.yml",
    "text": "Adding packages with newpackages.yml\nWhen the openscapes image is used, we are in a conda env called ‘notebook’. We want to update that with the packages in newpackages.yml but need to get that file into the container. For now, I just hard code in the pip install commands.\nAdd to Docker file\n# it can't find new.yml in home/joyvan/.kernels\n# need to get that into the container somehow (git clone?)\n# RUN conda env update --file new.yml"
  },
  {
    "objectID": "ci/iopython-tf/instructions.html",
    "href": "ci/iopython-tf/instructions.html",
    "title": "Indian Ocean Summer Docker Images",
    "section": "",
    "text": "https://hub.docker.com/repository/docker/eeholmes/iopython-tf/general\nThe one to use is the dated one. The main tag doesn’t seem to always be recognized as a new tag when it changes."
  },
  {
    "objectID": "ci/iopython-tf/instructions.html#to-set-docker-tag-to-latest-commit",
    "href": "ci/iopython-tf/instructions.html#to-set-docker-tag-to-latest-commit",
    "title": "Indian Ocean Summer Docker Images",
    "section": "To set docker tag to latest commit",
    "text": "To set docker tag to latest commit\nSHA7=\"$(git rev-parse --short HEAD)\"\nDOCKER_TAG=$SHA7\nI am not doing that since this repo has lots of commits unrelated to the docker image."
  },
  {
    "objectID": "ci/iopython-tf/instructions.html#to-set-up-your-own-docker-repo",
    "href": "ci/iopython-tf/instructions.html#to-set-up-your-own-docker-repo",
    "title": "Indian Ocean Summer Docker Images",
    "section": "To set up your own Docker repo",
    "text": "To set up your own Docker repo\n\nMake an account on DockerHub. Free is fine.\nCreate a repo and give it a name. For example, for this project, my account is eeholmes and my repo is iopython (Indian Ocean Python) as it is specific a particular project I am working on.\n\nDockerHub will want to you to buy the premium account but you only need that if you are doing continuous integration, like using a GitHub Action to autobuild your image. If you are manually building, you don’t need this."
  },
  {
    "objectID": "ci/iopython-tf/instructions.html#why-is---platform-needed-in-the-build-command",
    "href": "ci/iopython-tf/instructions.html#why-is---platform-needed-in-the-build-command",
    "title": "Indian Ocean Summer Docker Images",
    "section": "Why is --platform needed in the build command",
    "text": "Why is --platform needed in the build command\nYou won’t see this on docker build tutorials. But if you are on a Mac with Apple chip, then you’ll build arm64 images and that’s not going to work on Ubuntu VMs. The vanilla images you see are amd64 so we want to make sure we are building for that platform. This only matters if you are on a Mac with Apple chip, but it won’t break things for unix and PC so I added to make the instructions more robust."
  },
  {
    "objectID": "ci/iopython-tf/instructions.html#if-an-specific-image-tag-is-in-config",
    "href": "ci/iopython-tf/instructions.html#if-an-specific-image-tag-is-in-config",
    "title": "Indian Ocean Summer Docker Images",
    "section": "If an specific image tag is in config",
    "text": "If an specific image tag is in config\nThe JupyterHub has a config file that specifies what images are being used. If the image is say eeholmes/iopython:hublatest, then whenever the a image with tag hublatest is pushed, the hub will use that. If on the otherhand, you config file has a specific, an unique tag that you don’t overwrite, then you’ll have to update the file in the config file on the cluster (log into Azure, go to cluster, connect to cloud shell, nano dconfig2.yaml) and upgrade the installation of the JupyterHub.\nWhy not eeholmes/iopython:latest? There is nothing special about latest. It is the default tag used if you don’t specify -t and : in your build call. So it is a bit too easy to accidentally update “latest” and thus update the image for you hub when you didn’t intend to do that. You just forgot to specify a tag.\nTo update if you are using a specific tag, like 20230615 rather than one you keep updating like hublatest or latest:\n\nStep 1\nEdit the config file. Mine is called dconfig2.yaml. Yours is probably config.yaml. Name is unimportant.\nnano dconfig2.yaml\nInside dconfig2.yaml is this info. This shows a fixed tag. So if I update, I need to change the 20230615 part.\n  singleuser:\n    image:\n      name: eeholmes/iopython\n      tag: 20230615\nSave the changes. In nano, it cmd-O, return, cmd-X.\n\n\nrun helm upgrade\nhelm upgrade --cleanup-on-fail --render-subchart-notes dhub dask/daskhub --namespace dhub --version=2023.1.0 --values dconfig2.yaml\n\n\nThe helm upgrade command\nhelm upgrade --cleanup-on-fail --render-subchart-notes dhub dask/daskhub --namespace dhub --version=2023.1.0 --values dconfig2.yaml\nA helm is what runs the commands to upgrade (and install in the beginning) our JupyterHub. dask/daskhub is point to the repo with the “helm chart” (the instructions). --value dconfig2.yaml is telling it where the config file is.\n\nupgrade upgrade an existing installation with the values in dconfig2.yaml\n--render-subchart-notes the dask/daskhub helm chart has subcharts (jupyterhub) and you need to render these too. Not all helm charts have this.\ndask/daskhub the name of the repo that has the helm chart. The first time you reference this, you need to tell help about the repo by giving it the url. Read how here\n--version=2023.1.0 version of the helm chart. Update when the helm chart (instructions for installing the jupyterhub) changes."
  },
  {
    "objectID": "ci/iopython-tf/instructions.html#adding-packages-with-newpackages.yml",
    "href": "ci/iopython-tf/instructions.html#adding-packages-with-newpackages.yml",
    "title": "Indian Ocean Summer Docker Images",
    "section": "Adding packages with newpackages.yml",
    "text": "Adding packages with newpackages.yml\nWhen the openscapes image is used, we are in a conda env called ‘notebook’. We want to update that with the packages in newpackages.yml but need to get that file into the container. For now, I just hard code in the pip install commands.\nAdd to Docker file\n# it can't find new.yml in home/joyvan/.kernels\n# need to get that into the container somehow (git clone?)\n# RUN conda env update --file new.yml"
  },
  {
    "objectID": "ci/iosdmTMB/instructions.html",
    "href": "ci/iosdmTMB/instructions.html",
    "title": "Indian Ocean Summer Docker Images",
    "section": "",
    "text": "https://hub.docker.com/repository/docker/eeholmes/iosdmTMB/general\nA separate sdmTMB image is made since it takes forever to build."
  },
  {
    "objectID": "ci/iosdmTMB/instructions.html#to-set-docker-tag-to-latest-commit",
    "href": "ci/iosdmTMB/instructions.html#to-set-docker-tag-to-latest-commit",
    "title": "Indian Ocean Summer Docker Images",
    "section": "To set docker tag to latest commit",
    "text": "To set docker tag to latest commit\nSHA7=\"$(git rev-parse --short HEAD)\"\nDOCKER_TAG=$SHA7\nI am not doing that since this repo has lots of commits unrelated to the docker image."
  },
  {
    "objectID": "ci/iosdmTMB/instructions.html#to-set-up-your-own-docker-repo",
    "href": "ci/iosdmTMB/instructions.html#to-set-up-your-own-docker-repo",
    "title": "Indian Ocean Summer Docker Images",
    "section": "To set up your own Docker repo",
    "text": "To set up your own Docker repo\n\nMake an account on DockerHub. Free is fine.\nCreate a repo and give it a name. For example, for this project, my account is eeholmes and my repo is iopython (Indian Ocean Python) as it is specific a particular project I am working on.\n\nDockerHub will want to you to buy the premium account but you only need that if you are doing continuous integration, like using a GitHub Action to autobuild your image. If you are manually building, you don’t need this."
  },
  {
    "objectID": "ci/iosdmTMB/instructions.html#why-is---platform-needed-in-the-build-command",
    "href": "ci/iosdmTMB/instructions.html#why-is---platform-needed-in-the-build-command",
    "title": "Indian Ocean Summer Docker Images",
    "section": "Why is --platform needed in the build command",
    "text": "Why is --platform needed in the build command\nYou won’t see this on docker build tutorials. But if you are on a Mac with Apple chip, then you’ll build arm64 images and that’s not going to work on Ubuntu VMs. The vanilla images you see are amd64 so we want to make sure we are building for that platform. This only matters if you are on a Mac with Apple chip, but it won’t break things for unix and PC so I added to make the instructions more robust."
  },
  {
    "objectID": "ci/iosdmTMB/instructions.html#if-an-specific-image-tag-is-in-config",
    "href": "ci/iosdmTMB/instructions.html#if-an-specific-image-tag-is-in-config",
    "title": "Indian Ocean Summer Docker Images",
    "section": "If an specific image tag is in config",
    "text": "If an specific image tag is in config\nThe JupyterHub has a config file that specifies what images are being used. If the image is say eeholmes/iopython:hublatest, then whenever the a image with tag hublatest is pushed, the hub will use that. If on the otherhand, you config file has a specific, an unique tag that you don’t overwrite, then you’ll have to update the file in the config file on the cluster (log into Azure, go to cluster, connect to cloud shell, nano dconfig2.yaml) and upgrade the installation of the JupyterHub.\nWhy not eeholmes/iopython:latest? There is nothing special about latest. It is the default tag used if you don’t specify -t and : in your build call. So it is a bit too easy to accidentally update “latest” and thus update the image for you hub when you didn’t intend to do that. You just forgot to specify a tag.\nTo update if you are using a specific tag, like 20230615 rather than one you keep updating like hublatest or latest:\n\nStep 1\nEdit the config file. Mine is called dconfig2.yaml. Yours is probably config.yaml. Name is unimportant.\nnano dconfig2.yaml\nInside dconfig2.yaml is this info. This shows a fixed tag. So if I update, I need to change the 20230615 part.\n  singleuser:\n    image:\n      name: eeholmes/iopython\n      tag: 20230615\nSave the changes. In nano, it cmd-O, return, cmd-X.\n\n\nrun helm upgrade\nhelm upgrade --cleanup-on-fail --render-subchart-notes dhub dask/daskhub --namespace dhub --version=2023.1.0 --values dconfig2.yaml\n\n\nThe helm upgrade command\nhelm upgrade --cleanup-on-fail --render-subchart-notes dhub dask/daskhub --namespace dhub --version=2023.1.0 --values dconfig2.yaml\nA helm is what runs the commands to upgrade (and install in the beginning) our JupyterHub. dask/daskhub is point to the repo with the “helm chart” (the instructions). --value dconfig2.yaml is telling it where the config file is.\n\nupgrade upgrade an existing installation with the values in dconfig2.yaml\n--render-subchart-notes the dask/daskhub helm chart has subcharts (jupyterhub) and you need to render these too. Not all helm charts have this.\ndask/daskhub the name of the repo that has the helm chart. The first time you reference this, you need to tell help about the repo by giving it the url. Read how here\n--version=2023.1.0 version of the helm chart. Update when the helm chart (instructions for installing the jupyterhub) changes."
  },
  {
    "objectID": "ci/iosdmTMB/instructions.html#adding-packages-with-newpackages.yml",
    "href": "ci/iosdmTMB/instructions.html#adding-packages-with-newpackages.yml",
    "title": "Indian Ocean Summer Docker Images",
    "section": "Adding packages with newpackages.yml",
    "text": "Adding packages with newpackages.yml\nWhen the openscapes image is used, we are in a conda env called ‘notebook’. We want to update that with the packages in newpackages.yml but need to get that file into the container. For now, I just hard code in the pip install commands.\nAdd to Docker file\n# it can't find new.yml in home/joyvan/.kernels\n# need to get that into the container somehow (git clone?)\n# RUN conda env update --file new.yml"
  },
  {
    "objectID": "ci/iopython/instructions.html",
    "href": "ci/iopython/instructions.html",
    "title": "Indian Ocean Summer Docker Images: Openscapes + a few extras",
    "section": "",
    "text": "https://hub.docker.com/repository/docker/eeholmes/iopython/general\nThe one to use is the dated one. The main tag doesn’t seem to always be recognized as a new tag when it changes."
  },
  {
    "objectID": "ci/iopython/instructions.html#to-set-docker-tag-to-latest-commit",
    "href": "ci/iopython/instructions.html#to-set-docker-tag-to-latest-commit",
    "title": "Indian Ocean Summer Docker Images: Openscapes + a few extras",
    "section": "To set docker tag to latest commit",
    "text": "To set docker tag to latest commit\nSHA7=\"$(git rev-parse --short HEAD)\"\nDOCKER_TAG=$SHA7\nI am not doing that since this repo has lots of commits unrelated to the docker image."
  },
  {
    "objectID": "ci/iopython/instructions.html#to-set-up-your-own-docker-repo",
    "href": "ci/iopython/instructions.html#to-set-up-your-own-docker-repo",
    "title": "Indian Ocean Summer Docker Images: Openscapes + a few extras",
    "section": "To set up your own Docker repo",
    "text": "To set up your own Docker repo\n\nMake an account on DockerHub. Free is fine.\nCreate a repo and give it a name. For example, for this project, my account is eeholmes and my repo is iopython (Indian Ocean Python) as it is specific a particular project I am working on.\n\nDockerHub will want to you to buy the premium account but you only need that if you are doing continuous integration, like using a GitHub Action to autobuild your image. If you are manually building, you don’t need this."
  },
  {
    "objectID": "ci/iopython/instructions.html#why-is---platform-needed-in-the-build-command",
    "href": "ci/iopython/instructions.html#why-is---platform-needed-in-the-build-command",
    "title": "Indian Ocean Summer Docker Images: Openscapes + a few extras",
    "section": "Why is --platform needed in the build command",
    "text": "Why is --platform needed in the build command\nYou won’t see this on docker build tutorials. But if you are on a Mac with Apple chip, then you’ll build arm64 images and that’s not going to work on Ubuntu VMs. The vanilla images you see are amd64 so we want to make sure we are building for that platform. This only matters if you are on a Mac with Apple chip, but it won’t break things for unix and PC so I added to make the instructions more robust."
  },
  {
    "objectID": "ci/iopython/instructions.html#if-a-specific-image-tag-is-in-config",
    "href": "ci/iopython/instructions.html#if-a-specific-image-tag-is-in-config",
    "title": "Indian Ocean Summer Docker Images: Openscapes + a few extras",
    "section": "If a specific image tag is in config",
    "text": "If a specific image tag is in config\nThe JupyterHub has a config file that specifies what images are being used. If the image is say eeholmes/iopython:hublatest, then whenever the a image with tag hublatest is pushed, the hub will use that. If on the otherhand, you config file has a specific, an unique tag that you don’t overwrite, then you’ll have to update the file in the config file on the cluster (log into Azure, go to cluster, connect to cloud shell, nano dconfig2.yaml) and upgrade the installation of the JupyterHub.\nWhy not eeholmes/iopython:latest? There is nothing special about latest. It is the default tag used if you don’t specify -t and : in your build call. So it is a bit too easy to accidentally update “latest” and thus update the image for you hub when you didn’t intend to do that. You just forgot to specify a tag.\nTo update if you are using a specific tag, like 20230615 rather than one you keep updating like hublatest or latest:\n\nStep 1\nEdit the config file. Mine is called dconfig2.yaml. Yours is probably config.yaml. Name is unimportant.\nnano dconfig2.yaml\nInside dconfig2.yaml is this info. This shows a fixed tag. So if I update, I need to change the 20230615 part.\n  singleuser:\n    image:\n      name: eeholmes/iopython\n      tag: 20230615\nSave the changes. In nano, it cmd-O, return, cmd-X.\n\n\nrun helm upgrade\nhelm upgrade --cleanup-on-fail --render-subchart-notes dhub dask/daskhub --namespace dhub --version=2023.1.0 --values dconfig2.yaml\n\n\nThe helm upgrade command\nhelm upgrade --cleanup-on-fail --render-subchart-notes dhub dask/daskhub --namespace dhub --version=2023.1.0 --values dconfig2.yaml\nA helm is what runs the commands to upgrade (and install in the beginning) our JupyterHub. dask/daskhub is point to the repo with the “helm chart” (the instructions). --value dconfig2.yaml is telling it where the config file is.\n\nupgrade upgrade an existing installation with the values in dconfig2.yaml\n--render-subchart-notes the dask/daskhub helm chart has subcharts (jupyterhub) and you need to render these too. Not all helm charts have this.\ndask/daskhub the name of the repo that has the helm chart. The first time you reference this, you need to tell help about the repo by giving it the url. Read how here\n--version=2023.1.0 version of the helm chart. Update when the helm chart (instructions for installing the jupyterhub) changes."
  },
  {
    "objectID": "ci/iopython/instructions.html#adding-packages-with-newpackages.yml",
    "href": "ci/iopython/instructions.html#adding-packages-with-newpackages.yml",
    "title": "Indian Ocean Summer Docker Images: Openscapes + a few extras",
    "section": "Adding packages with newpackages.yml",
    "text": "Adding packages with newpackages.yml\nWhen the openscapes image is used, we are in a conda env called ‘notebook’. We want to update that with the packages in newpackages.yml but need to get that file into the container. For now, I just hard code in the pip/conda install commands.\nAdd to Docker file\n# it can't find new.yml in home/joyvan/.kernels\n# need to get that into the container somehow (git clone?)\n# RUN conda env update --file new.yml"
  },
  {
    "objectID": "Set-up-centos.html",
    "href": "Set-up-centos.html",
    "title": "Centos Set-up",
    "section": "",
    "text": "This is my notes for setting this up on a Centos 7 (Linux distribution) server. Jump to the “Summary” section to see only the instructions without explanations.\nAll the commands are run in a shell (bash)\nReferences:"
  },
  {
    "objectID": "Set-up-centos.html#set-up-vm-on-azure",
    "href": "Set-up-centos.html#set-up-vm-on-azure",
    "title": "Centos Set-up",
    "section": "Set up VM on Azure",
    "text": "Set up VM on Azure\n\nCreated a Centos 8.3 server on Azure: https://portal.azure.com/#create/cloud-infrastructure-services.centos-8-3centos-8-3\nI didn’t do anything special for set-up. Choose SSH with key.\nOnce it is created, I went to the dashboard and selected my VM. The dashboard has a “Connect” button to get to the shell and it shows the public IP address.\nI had to create a special security rule to allow me to ssh into the public IP address to connect. Normally I use the cloud shell to connect, but Azure would not let me connect via the cloud shell for a server since it wanted upgraded security and I cannot do that with my work subscription.\nThen I saved the key somewhere on my computer and\n\n\nchmod 400 ~/&lt;key location&gt;\nssh -i ~/&lt;key location&gt; &lt;vm-username&gt;@&lt;public key&gt;"
  },
  {
    "objectID": "Set-up-centos.html#on-vm-check-set-up",
    "href": "Set-up-centos.html#on-vm-check-set-up",
    "title": "Centos Set-up",
    "section": "On VM check set-up",
    "text": "On VM check set-up\nI ssh-ed into the VM with\n\nssh -i &lt;path to key downloaded from Azure&gt; eeholmes@&lt;public ip address&gt;\n\n\nMake sure you are root\nGetting the JupyterHub set up needs to be done as root. First make sure you have an admin password. When I set up my Azure VM, I did not set a password. So first\n\nsudo passwd &lt;your username&gt;\n\nand set a password. Then switch to root if you are not signed in as root\n\nsudo -i\n\n\n\nCheck for Python\nYou will need Python 3.6+ installed. Open a terminal window and run python3 --version or python --version to see if Python is installed and what the version is.\nCheck your operating system (OS) with\n\ncat /etc/os-release\n\n\n\nCheck for conda\nYou will need conda (or miniconda) for these instructions. conda (and miniconda) take care of checking that all our packages will be inter-operable. It is best to install JupyterHub into a clean environment. That way you minimize chances of conflicts and your environment will solve (figure out any conflicts) much much faster.\nCheck for conda with\n\nconda list\n\nIf it doesn’t show a list of environments, then you need to install miniconda. Installation instructions. Read about miniconda for scientists from Software Carpentries here.\nThis is what I used to install miniconda from these instructions. Note install miniconda in some place like /opt/miniconda3 where all users will have access to `/opt/miniconda3/bin. We don’t want to install in /root/ for example or the admin users home directory.\n\nmkdir -p /opt/miniconda3\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O /opt/miniconda3/miniconda.sh\nbash /opt/miniconda3/miniconda.sh -b -u -p /opt/miniconda3\nrm -rf /opt/miniconda3/miniconda.sh\n\nThen initialize to set up the path. Note I am using bash. You’ll need to change if you are using zsh.\n\n/opt/miniconda3/bin/conda init bash\nsource ~/.bashrc\n\nnote will need to do something else to add the conda binary to all the users’ paths"
  },
  {
    "objectID": "Set-up-centos.html#create-the-conda-environment",
    "href": "Set-up-centos.html#create-the-conda-environment",
    "title": "Centos Set-up",
    "section": "Create the conda environment",
    "text": "Create the conda environment\nCreate the conda environment for the jupyterhub installation. Installation will be in a directory with all the files for packages. Then activate it (enter it), and get the location of the environment (folder).\nAll the commands below are in the terminal window on your VM/server.\nCreate the environment named jupyterhub with python and jupyterhub (module). After creating, activate (enter) that environment. Then install jupyterlab, notebook and dockerspawner into the environment. Note the jupyterhub after -n is the name of the environment.\n\nconda create -n jupyterhub python\n\nThen activate (enter) that environment\n\nconda activate jupyterhub\n\nThen install jupyterhub here\n\nconda install -c conda-forge jupyterhub\n\nand then jupyterlab\n\nconda install -c conda-forge jupyterlab notebook\n\n\nSet a variable for env path\nThe environment has a folder with all the packages and binaries that we install. We are going to need to know the location of that folder. Get the location with\n\nconda env list\n\nOn the VM I set up, the folder location is\n\n/opt/miniconda3/envs/jupyterhub\n\nYours could be something entirely different. On another server with anaconda (a not-free conda package resolver), the folder was\n\n/SHARE/anaconda3/envs/jupterhub/\n\nWe are going to be saving the configuration files for our JupyterHub in this folder. Let’s save the path to a variable so we don’t have to keep entering the whole path.\n\nJHUBENV=/opt/miniconda3/envs/jupyterhub\n\nMake sure users can read and execute this folder. They need to in order to be able to spawn instances for the hub.\n\nchmod 755 $JHUBENV\n\nYou should now be able to start the hub, but you will not be able to access it yet because you need to open the 8000 port. Type\n\n$JHUBENV/bin/jupyterhub\n\nand check that it starts. Then use Cntl-C to stop the hub."
  },
  {
    "objectID": "Set-up-centos.html#create-a-user-on-the-vm",
    "href": "Set-up-centos.html#create-a-user-on-the-vm",
    "title": "Centos Set-up",
    "section": "Create a user on the VM",
    "text": "Create a user on the VM\nBy default, any user on the server will be able to login. Let’s create a test user so that we are not logging into our hub with the root user password. We will be using “http” until we secure it so passwords are potentially exposed.\n\nuseradd jhub\n\nand give it a password when it asks."
  },
  {
    "objectID": "Set-up-centos.html#open-the-8000-port",
    "href": "Set-up-centos.html#open-the-8000-port",
    "title": "Centos Set-up",
    "section": "Open the 8000 port",
    "text": "Open the 8000 port\nFirewallD was not running on my Azure Centos server, so I started it up to manage the ports.\n\nsudo systemctl enable firewalld\nsudo systemctl start firewalld\n\nFind out the Public IP address for the server you are on; it’s listed on the Azure overview and networking page for the VM in the Azure portal. Then open the 8000 port.\nFirst find out what ports are open through the firewall\n\nsudo firewall-cmd --list-ports\n\nAdd the 8000 port, reload and recheck that it appears.\n\nsudo firewall-cmd  --permanent --add-port 8000/tcp\nsudo firewall-cmd --reload\nsudo firewall-cmd --list-ports\n\nBecause I am on an Azure VM, I also have to set up a networking rule to allow the 8000 port. By default, all public access to the server is blocked. Go to the Azure dashboard, select your VM, then select Networking under Settings, and then click Add Inbound Port rule. I am pretty sure you need to select “http” instead of “https”.\nOnce the port is open, you should be able to reach your JupyterHub at http://XXX.XX.XX.XX:8000 (replace the XX’s with the Public IP address).\nBackground\nThe JupyterhHub is running by default on http://localhost:8000. This means that if you start the hub on a machine that you are logged into, you should be able to open a browser on that machine, enter http://localhost:8000 and the hub login page will appear. There are a few reasons that might not work\n\nYou are ssh-ing into a server and don’t have a browser to open. The browser on the computer that you are ssh-ing from is the “localhost” in this case and you need the “localhost” to be the server.\nYou are logged directly into your server, but it doesn’t have a browser installed.\n\nHowever http://localhost:8000 is actually not very useful. We are trying to create a hub that others can log into from their browsers.\nSo you need to determine the Public IP address for the server you are on. This is the IP address that you could enter into a browser. If you enter http://XXX.XX.XX.XX (replace with actual IP), then you should see a page of some sort. This indicates that the server is working. If you are on an internal network, then you will only be able to load the address if you are also on that network. But for security reason, ports will not be open by default. You need to open the 8000 port so that http://XXX.XX.XX.XX:8000 will be found."
  },
  {
    "objectID": "Set-up-centos.html#log-in",
    "href": "Set-up-centos.html#log-in",
    "title": "Centos Set-up",
    "section": "Log in!",
    "text": "Log in!\nAt this point, you should be able to login with the jhub test account."
  },
  {
    "objectID": "Set-up-centos.html#set-up-a-configuration-file",
    "href": "Set-up-centos.html#set-up-a-configuration-file",
    "title": "Centos Set-up",
    "section": "Set up a configuration file",
    "text": "Set up a configuration file\nSo far, we have started the hub with the default configuration. We are going to need to customize it. For that we need a configuration file. We will create this in the folder where the environment files are.\n\nsudo mkdir -p $JHUBENV/etc/jupyterhub/\ncd $JHUBENV/etc/jupyterhub/\n\nNext create the default configuration file jupyterhub_config.py.\n\nsudo $JHUBENV/bin/jupyterhub --generate-config\n\nBecause we cd-d into the $JHUBENV/etc/jupyterhub/ directory, the file is created there. This default file is very long. Open up with\n\nnano jupyterhub_config.py\n\nUse F6 to find lines. Uncomment these two lines and save (Cntl-O, Enter, Cntl-X).\n\nc.Spawner.http_timeout = 3600"
  },
  {
    "objectID": "Set-up-centos.html#make-a-new-server-service",
    "href": "Set-up-centos.html#make-a-new-server-service",
    "title": "Centos Set-up",
    "section": "Make a new server service",
    "text": "Make a new server service\n\nCreate the new unit file\nAt this point, after opening the port, you should be able to get to your JupyterHub by starting it with jupyterhub --ip XXX.XX.XX.XX --port=8000 and then browsing to http://XXX.XX.XX.XX:8000. But you hub is going to be stopped whenever the server is rebooted. So next we need to set up a service for your service so that our hub starts automatically.\nCreate a new directory for the service unit file,\n\nsudo mkdir -p $JHUBENV/etc/systemd\ncd $JHUBENV/etc/systemd\n\nCreate the file and name jupyterhub.service. For example, using nano editor, we do\n\nnano jupyterhub.service\n\nAnd into that file we put the following. Replace /opt/miniconda3/envs/jupyterhub with the actual path to the jupyterhub environment folder.\n\n[Unit]\nDescription=JupyterHub\nAfter=syslog.target network.target\n\n[Service]\nUser=root\nEnvironment=\"PATH=/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/miniconda3/envs/jupyterhub/bin\"\nExecStart=/opt/miniconda3/envs/jupyterhub/bin/jupyterhub -f /opt/miniconda3/envs/jupyterhub/etc/jupyterhub/jupyterhub_config.py\n\n[Install]\nWantedBy=multi-user.target\n\nNext we make systemd aware of the new service.\nCreate a symlink file in the folder where all the server services are kept. And tell systemd to reload its configuration files\n\nsudo ln -s $JHUBENV/etc/systemd/jupyterhub.service /etc/systemd/system/jupyterhub.service\nsudo systemctl daemon-reload\n\n\n\nMake sure SELinux doesn’t block our service\nSELinux (security for the server) checks that files that are used have the correct label. All our files have generic file labels. If you do,\n\nls -Z $JHUBENV/etc/systemd/\n\nYou will see that the file label is unconfined_u:object_r:usr_t:s0. We need it to be\n\nsystemd_unit_file_t\n\nWe change the file label with\n\nsudo chcon system_u:object_r:systemd_unit_file_t:s0 $JHUBENV/etc/systemd/jupyterhub.service\n\nSELinux will also object to the file label on all the binaries that we use to start up the JupyterHub (like jupyterhub) so we need to fix those file labels.\nThis will add bin_t label to all the binaries and check that it worked.\n\nsudo find $JHUBENV/bin -type f -exec chcon system_u:object_r:bin_t:s0 {} \\;\nls -Z $JHUBENV/bin\n\nIt got all the binaries but not the simlinks. Nonetheless it seemed to run ok."
  },
  {
    "objectID": "Set-up-centos.html#enable-our-new-service",
    "href": "Set-up-centos.html#enable-our-new-service",
    "title": "Centos Set-up",
    "section": "Enable our new service",
    "text": "Enable our new service\n\nsudo systemctl enable jupyterhub.service\n\nThe service will start on reboot, but we can start it straight away using start:\n\nsudo systemctl start jupyterhub.service\n\nCheck that it is running.\n\nsudo systemctl status jupyterhub.service\n\nIf it fails, try\n\naudit2why &lt; /var/log/audit/audit.log\n\nto debug. It is likely to be an issue with SELinux blocking the service from starting.\nNow our hub should be available on http:\\\\XXX.XX.XX.XX:8000. You can double check that it is listen on this port by running\n\nnetstat -tuln\n\nAt this point, you will need to address security if your hub is open to the web, as opposed to being on an internal network and only accessible to that network. Learn about that here."
  },
  {
    "objectID": "Set-up-centos.html#create-the-base-user-environment",
    "href": "Set-up-centos.html#create-the-base-user-environment",
    "title": "Centos Set-up",
    "section": "Create the base user environment",
    "text": "Create the base user environment\nWhen you log in the jupyter notebooks will be trying to use the Python environment that was created to install JupyterHub, this is not what we want. We will use a docker image to “spawn” the user environment. Read here for other approaches.\nWe are going to use dockerspawner so that we can use a docker image for our user environments. The user will work in these containerized environments and they won’t have access to any other files in the server. In order to share their work with others, the normal workflow would be to work in Git repos and share those repos to a GitHub (or GitLab server). Each user will have a home directory on the server for their files, but they won’t have access to other hub user directories nor will they have access to any other directories on the server.\n\nInstall docker\nI am using Centos in this example\n\nsudo yum install -y yum-utils\nsudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo\n\nThen we need to start docker\n\nsudo systemctl start docker\n\nNot sure this is needed.\n\nsudo firewall-cmd --zone=docker --add-port=8081/tcp\nsudo firewall-cmd --reload\nsudo systemctl restart docker\n\n\n\nInstall dockerspawner\nI am going to be creating the user environment from a docker image, so I also want dockerspawner. Note dockerspawner installed docker-py but it was really old and threw errors so I installed separately to get the latest version. Note make sure you are in the jupyterhub conda env. You can run conda env list and use conda activate jupyterhub if you are not in it.\n\nconda install -c conda-forge dockerspawner\nconda install -c conda-forge docker-py\n\n\n\nJupyter images\nThe image that we use must have the jupyterhub module installed. Also there seems to be some tweaking needed for the image to work as the image I used in a JupyterHub on Kubernetes did not work for me.\nCheck the version on your server:\n\n$JHUBENV/bin/jupyterhub -V\n\nFor demo purposes, we will use the jupyter images on DockerHub. We want to find an image with the same version of jupyterhub as we have on our server.\n\n\nEdit the config file\nEdit the jupyterhub_config.py file in $JHUB-ENV/etc/jupyterhub/ to add that we want to use DockerSpawner and specify the images that users should have access to. Users will get a drop down menu. Add these lines to jupyterhub_config.py. The hub bind url needs to be 0.0.0.0 because we are using a docker container for the individual user environments.\n\nhttps://discourse.jupyter.org/t/whats-the-main-difference-between-hub-connect-url-vs-hub-bind-url/3596/2\n\n\nc = get_config()  #noqa\nc.JupyterHub.port = 8000\nc.JupyterHub.hub_bind_url = \"http://0.0.0.0:8081\"\nc.JupyterHub.spawner_class = 'dockerspawner.DockerSpawner'\nc.DockerSpawner.remove = True\nc.Spawner.http_timeout = 3600\nc.DockerSpawner.image_whitelist = {\n    'datascience-r': 'jupyter/datascience-notebook:r-4.3.1',\n    'scipy-notebook': 'jupyter/scipy-notebook:7e1a19a8427f',\n}\n\nDo a docker pull of the images so that they don’t have to be pulled the first time that a user chooses that image.\n\ndocker pull jupyter/datascience-notebook:r-4.3.1\ndocker pull jupyter/scipy-notebook:7e1a19a8427f\n\nNow you can restart the service and the user can start a notebook with the specified images."
  },
  {
    "objectID": "Set-up-centos.html#summary",
    "href": "Set-up-centos.html#summary",
    "title": "Centos Set-up",
    "section": "Summary",
    "text": "Summary\nOnly the instructions. Make sure you are installing as the root user. I assume you have Python and conda installed.\nCreate the conda environment\n\nsudo -i\n\nconda create -n jupyterhub python --yes\nconda activate jupyterhub\nconda install -c conda-forge jupyterhub --yes\nconda install -c conda-forge jupyterlab notebook --yes\n\nJHUBENV=/opt/miniconda3/envs/jupyterhub\nchmod 755 $JHUBENV\n\nCreate user\n\nuseradd jhub\n\nOpen the 8000 port\n\n#sudo systemctl enable firewalld\n#sudo systemctl start firewalld\n\nsudo firewall-cmd  --permanent --add-port 8000/tcp\nsudo firewall-cmd --reload\nsudo firewall-cmd --list-ports\n\nCreate the configuration file. Will be editted at end.\n\nsudo mkdir -p $JHUBENV/etc/jupyterhub/\ncd $JHUBENV/etc/jupyterhub/\nsudo $JHUBENV/bin/jupyterhub --generate-config\n\nInstall docker if needed\n\nsudo yum install -y yum-utils\nsudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo\n\nsudo systemctl start docker\n\nNot sure this is needed.\n\nsudo firewall-cmd --zone=docker --add-port=8081/tcp\nsudo firewall-cmd --reload\nsudo systemctl restart docker\n\nInstall dockerspawner\n\nconda install -c conda-forge dockerspawner --yes\nconda install -c conda-forge docker-py --yes\n\nEdit the configuration file.\n\ncd $JHUBENV/etc/jupyterhub/\nnano jupyterhub_config.py\n\nPaste this in\n\nc = get_config()  #noqa\nc.JupyterHub.port = 8000\nc.JupyterHub.hub_bind_url = \"http://0.0.0.0:8081\"\nc.JupyterHub.spawner_class = 'dockerspawner.DockerSpawner'\nc.DockerSpawner.remove = True\nc.Spawner.http_timeout = 3600\nc.DockerSpawner.image_whitelist = {\n    'datascience-r': 'jupyter/datascience-notebook:r-4.3.1',\n    'scipy-notebook': 'jupyter/scipy-notebook:7e1a19a8427f',\n}\n\nDocker pull of the images\n\ndocker pull jupyter/datascience-notebook:r-4.3.1\ndocker pull jupyter/scipy-notebook:7e1a19a8427f\n\nMake a new server service\n\nsudo mkdir -p $JHUBENV/etc/systemd\ncd $JHUBENV/etc/systemd\nnano jupyterhub.service\n\nPaste this in\n\n[Unit]\nDescription=JupyterHub\nAfter=syslog.target network.target\n\n[Service]\nUser=root\nEnvironment=\"PATH=/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/miniconda3/envs/jupyterhub/bin\"\nExecStart=/opt/miniconda3/envs/jupyterhub/bin/jupyterhub -f /opt/miniconda3/envs/jupyterhub/etc/jupyterhub/jupyterhub_config.py\n\n[Install]\nWantedBy=multi-user.target\n\nMake sure SELinux doesn’t block our service\n\nls -Z $JHUBENV/etc/systemd/\nsudo chcon system_u:object_r:systemd_unit_file_t:s0 $JHUBENV/etc/systemd/jupyterhub.service\nsudo find $JHUBENV/bin -type f -exec chcon system_u:object_r:bin_t:s0 {} \\;\n\nEnable our new service\n\nsudo ln -s $JHUBENV/etc/systemd/jupyterhub.service /etc/systemd/system/jupyterhub.service\nsudo systemctl daemon-reload\nsudo systemctl enable jupyterhub.service\nsudo systemctl start jupyterhub.service\n\nDone! See the long instructions if anything is not working."
  },
  {
    "objectID": "JHub-User-Guide.html",
    "href": "JHub-User-Guide.html",
    "title": "JHub User Instructions",
    "section": "",
    "text": "I have set us up a JupyterHub/RStudio cloud-computing hub on Azure. It’s on Kubernetes and will spin up VMs as needed. The VMs are not huge: 2CPU & 8 Gig RAM.\nhttps://jhub.opensci.live/hub/login\nLogin authentication is via GitHub. Only members of the JHub GitHub team can log on. This is a testing environment. Contact Eli if you want to test it out.\nInstructions: It should be pretty self-explanatory."
  },
  {
    "objectID": "JHub-User-Guide.html#login",
    "href": "JHub-User-Guide.html#login",
    "title": "JHub User Instructions",
    "section": "Login",
    "text": "Login\nAfter you get paste the login page, you will see this. Choose Python for Python only; choose R for Python and R."
  },
  {
    "objectID": "JHub-User-Guide.html#spin-up-your-server",
    "href": "JHub-User-Guide.html#spin-up-your-server",
    "title": "JHub User Instructions",
    "section": "Spin up your server",
    "text": "Spin up your server\nYou will see this as your server spins up"
  },
  {
    "objectID": "JHub-User-Guide.html#choose-your-platform",
    "href": "JHub-User-Guide.html#choose-your-platform",
    "title": "JHub User Instructions",
    "section": "Choose your platform",
    "text": "Choose your platform\nYou can code in RStudio, JupyterLab or Terminal."
  },
  {
    "objectID": "JHub-User-Guide.html#lets-choose-rstudio",
    "href": "JHub-User-Guide.html#lets-choose-rstudio",
    "title": "JHub User Instructions",
    "section": "Let’s choose RStudio",
    "text": "Let’s choose RStudio\n\n\nClone a repo\nChoose ‘new project’ (top right) and Version Control.\n\n\n\nTell Git who you are\nTell Git who you are and save your authentication info. You only do this once (or until your PAT expires). Run this code from the R console (not terminal).\nusethis::use_git_config(user.name = \"YourName\", user.email = \"your@email.com\")\nNow create a personal access token for authentication. SAVE the token because you will need it in the next step.\nusethis::create_github_token() \nNow run this and paste in the token.\ngitcreds::gitcreds_set()\nRestart R. You can chose your project from the dropdown on the top right to do this.\nNow commit a change and push.\nNote, you can also run the commands below from a terminal window.\ngit config --global user.name \"YourName\"\ngit config --global user.email \"your@email.com\"\ngit config --global credential.helper store"
  },
  {
    "objectID": "JHub-User-Guide.html#lets-choose-jupyterlab",
    "href": "JHub-User-Guide.html#lets-choose-jupyterlab",
    "title": "JHub User Instructions",
    "section": "Let’s choose JupyterLab",
    "text": "Let’s choose JupyterLab\nAny of the browser tabs with the Jupyter icon are JupyterLab.\nBe careful because you can be in RStudio with one GitHub repository and you could open the same repo in JupyterLab and easily create merge conflicts. Just be aware that they are in separate file systems so changes on RStudio will not be reflected in JupyterLab. It is not like you are on one computer.\n\nTell Git who you are\nBut I just did that with RStudio! I know but the JLab instance is in a different environment and doesn’t know what you did in the RStudio environment.\nOpen a terminal. You do this from the Launcher window. You can always open a new launcher window by clicking the little + tab\n\nNow click Terminal and run this code\ngit config --global user.name \"YourName\"\ngit config --global user.email \"your@email.com\"\ngit config --global credential.helper store\n\nCreate a PAT or use the one you created for RStudio\nMake a commit and push with the PAT as the password. Now you are set (until your PAT expires).\n\n\n\nClone a Git repo\nClick the Git icon on the left and you can clone a repo."
  },
  {
    "objectID": "JHub-User-Guide.html#stop-your-server",
    "href": "JHub-User-Guide.html#stop-your-server",
    "title": "JHub User Instructions",
    "section": "Stop your server",
    "text": "Stop your server\nIt will stop on it’s own after awhile, but if it hangs, you can stop it and restart it. With File &gt; Hub Control Panel"
  },
  {
    "objectID": "set-up-vm.html",
    "href": "set-up-vm.html",
    "title": "Set up VM",
    "section": "",
    "text": "For testing JupyterHub set-ups, I start various Linux machines. Here is how to set up a virtual machines."
  },
  {
    "objectID": "set-up-vm.html#azure",
    "href": "set-up-vm.html#azure",
    "title": "Set up VM",
    "section": "Azure",
    "text": "Azure\n\nCreated a Centos 8.3 server on Azure: https://portal.azure.com/#create/cloud-infrastructure-services.centos-8-3centos-8-3\nI didn’t do anything special for set-up. Choose SSH with key.\nOnce it is created, I went to the dashboard and selected my VM. The dashboard has a “Connect” button to get to the shell and it shows the public IP address.\nI had to create a special security rule to allow me to ssh into the public IP address to connect. Normally I use the cloud shell to connect, but Azure would not let me connect via the cloud shell for a server since it wanted upgraded security and I cannot do that with my work subscription.\nThen I saved the key somewhere on my computer and\n\nchmod 400 ~/&lt;key location&gt;\nssh -i ~/&lt;key location&gt; &lt;vm-username&gt;@&lt;public key&gt;\n\nI downloaded VMware Fusion 13.0.2 for M1 macs.\nThen I downloaded a Centos 9 server image from here\nhttps://www.centos.org/download/\nOpen VMWare and create a new VM. Choose other Linux. Doesn’t actually matter since it will be removed.\nShut down the VM.\nGo to settings and remove the hard drive.\nAdd a new hardrive. For me, I used ‘Add Device’ in the upper right of the Settings box. Choose ‘existing harddrive’\nHelp for M1 https://medium.com/@thehippieandtheboss/how-to-create-a-linux-virtual-machine-on-macos-1278ec1ef327\nhttps://tomcudd.com/how-i-set-up-a-centos-7-virtual-machine/"
  },
  {
    "objectID": "Setup-Notes.html",
    "href": "Setup-Notes.html",
    "title": "Instructions for editing config",
    "section": "",
    "text": "Instructions for editing config\n\nLog into https://portal.azure.com/ and once successful, you will see this\n\n\n\nClick the JupyterHub icon and you will see this\n\n\n\nClick the Connect icon and you will see this. Ignore everything else that you see. I don’t think you need to run the kubectl get deployments --all-namespaces=true unless you need to check Kubernetes set up.\n\n\n\nType nano config.yaml to get the the JupyterHub config. This is the only file you need to change. cntl-O to write. cntl-X to exit.\n\nAfter you update the config.yaml, you need to tell the JupyterHub about the change\nhelm upgrade --cleanup-on-fail jhub jupyterhub/jupyterhub --namespace jhub  --version=2.0.0 --values config.yaml\nIf upgrade was successful, you will see this (plus a bunch of text below that you can ignore).\n\n\nWhat a few minutes for your changes to take effect."
  }
]